
## 整理步骤
- trainer结构梳理
  - 源代码梳理
  - 细致摘录总结每一步操作
  - 粗略摘录总结核心步骤
  - 全局核心操作总结（例如"怎么做保存的"）

- 文档中 TrainingArguments每个参数的怎么发挥作用
- run_clm.py梳理
- run_clm_no_trainer.py梳理

TODO
- datasets

## 理论总结
### Trainer怎么控制log, save, evaluate的
- DefaultFlowCallback 设定了一些每个step和每个epoch结束时的行为
- 在训练的特殊节点都会尝试进行log_save_evaluate，是根据control变量的相关属性来决定是否分别做log_save_evaluate
- 而control变量的相关属性在 DefaultFlowCallback 的相关callback_step被调用时会被修改，例如on_step_end中会设置control.should_log, control.should_evaluate,而这些值的设定也是根据TrainingArgumens中的可配置参数设定的。
- 思考为什么要这么实现：**首先我们会通过TrainingArgumens设定什么时候进行log, save, evaluate；那么我们在Trainer.train的每个特殊节点都需要判断是否要进行log_save_evaluate，既然很多地方要重复写，那这三个操作可以抽象为一个函数_maybe_log_save_evaluate，但是这个函数需要一个信号——时机，因为不同的时机这个函数的具体操作是不同的，比如step_end要log，epoch_end不要log；所以定义一个全局的control变量来区分不同时机，不同的时机下control变量是怎么根据情况变化的呢？是通过callback_step让在不同的时机调整control的属性；callback_step在每个时机决定control变化的依据，当然是根据最初的TrainingArguments设定的参数！**



## Traier入参
- `data_collator` (DataCollator, optional) — The function to use to form a batch from a list of elements of train_dataset or eval_dataset. **Will default to default_data_collator(). if no tokenizer is provided, an instance of DataCollatorWithPadding otherwise.**
- `tokenizer` (PreTrainedTokenizerBase, optional) — The tokenizer used to preprocess the data. If provided, **will be used to automatically pad the inputs to the maximum length when batching inputs**, and it will be saved along the model to make it easier to rerun an interrupted training or reuse the fine-tuned model.
- `model_init`(Callable[[], PreTrainedModel], optional) — A function that instantiates the model to be used. If provided, **each call to train() will start from a new instance of the model as given by this function.**

- `compute_metrics` #TODO
- `optimizers` (Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR], optional, defaults to (None, None)) — A tuple containing the optimizer and the scheduler to use. Will default to an instance of AdamW on your model and a scheduler given by `get_linear_schedule_with_warmup()` controlled by args.
- preprocess_logits_for_metrics #TODO

### 重要属性
- `place_model_on_device` — Whether or not to automatically place the model on the device - **it will be set to False if model parallel or deepspeed is used**, or if the default TrainingArguments.place_model_on_device is overridden to return False .







## TrainingArguments参数梳理


- `eval_steps` (int or float, optional) 
    - Number of update steps between two evaluations if eval_strategy="steps". **Will default to the same value as logging_steps if not set.** Should be an integer or a float in range [0,1). If smaller than 1, will be interpreted as ratio of total training steps.

- `eval_strategy` (str or IntervalStrategy, optional, **defaults to "no"**)
    - The evaluation strategy to adopt during training. Possible values are:
        - "no": No evaluation is done during training.
        - "steps": Evaluation is done (and logged) every eval_steps.
        - "epoch": Evaluation is done at the end of each epoch.

- `save_steps` (int or float, optional, **defaults to 500**)
    - **根据update_step进行比较判断**
    - Number of updates steps before two checkpoint saves if save_strategy="steps". Should be an integer or a float in range [0,1). If smaller than 1, will be interpreted as ratio of total training steps.

- `save_strategy` (str or IntervalStrategy, optional, **defaults to "steps"**) 
    - The checkpoint save strategy to adopt during training. Possible values are:
        - "no": No save is done during training.
        - "epoch": Save is done at the end of each epoch.
        - "steps": Save is done every save_steps.
    - If "epoch" or "steps" is chosen, saving will also be performed at the very end of training, always.

- `save_total_limit` (int, optional) 
    - If a value is passed, will limit the total amount of checkpoints. Deletes the older checkpoints in output_dir. When load_best_model_at_end is enabled, the “best” checkpoint according to metric_for_best_model will always be retained in addition to the most recent ones. For example, **for save_total_limit=5 and load_best_model_at_end, the four last checkpoints will always be retained alongside the best model. When save_total_limit=1 and load_best_model_at_end, it is possible that two checkpoints are saved: the last one and the best one (if they are different).**

- `save_safetensors` (bool, optional, defaults to True)
    - Use safetensors saving and loading for state dicts instead of default torch.load and torch.save.

- `save_on_each_node` (bool, optional, defaults to False) 
    - When doing multi-node distributed training, whether to save models and checkpoints on each node, or only on the main one.
    - This should not be activated **when the different nodes use the same storage** as the files will be saved with the same names for each node.

- `save_only_model` (bool, optional, defaults to False) 
    - When checkpointing, whether to only save the model, or also the optimizer, scheduler & rng state. **Note that when this is true, you won’t be able to resume training from checkpoint**. This enables you to save storage by not storing the optimizer, scheduler & rng state. You can only load the model using from_pretrained with this option set to True.

- `restore_callback_states_from_checkpoint` (bool, optional, defaults to False)
    - Whether to restore the callback states from the checkpoint. If True, will override callbacks passed to the Trainer if they exist in the checkpoint.”
  
- `use_cpu` (bool, optional, defaults to False)
    - Whether or not to use cpu. If set to False, we will use cuda or mps device if available.

- `data_seed` (int, optional) 
    - Random seed to be used with data samplers. If not set, random generators for data sampling will use the same seed as seed. This can be used to ensure reproducibility of data sampling, independent of the model seed.

- `fp16` (bool, optional, defaults to False)
    - Whether to use fp16 16-bit (mixed) precision training instead of 32-bit training.

- `bf16` (bool, optional, defaults to False) 
    - Whether to use bf16 16-bit (mixed) precision training instead of 32-bit training. Requires **Ampere or higher NVIDIA architecture or using CPU (use_cpu) or Ascend NPU**. This is an experimental API and it may change.

- `half_precision_backend` (str, optional, defaults to "auto")
    - **The backend to use for mixed precision training**. Must be one of "auto", "apex", "cpu_amp". "auto" will use CPU/CUDA AMP or APEX depending on the PyTorch version detected, while the other choices will force the requested backend.

- `train_batch_size` 属性
    - 分布式训练时，就等于 `per_device_train_batch_size`


- `n_gpu` 属性
    - 当前进程使用的GPU数量，**分布式训练时，该属性总是固定为1**，只有在未使用分布式才可能大于1。


- `ignore_data_skip`  (bool, optional, defaults to False)
    - **断点续训时，是否要忽略已训数据跳过，默认为False，即不忽略，对已训过的部分进行跳过。**
    - When resuming training, whether or not to skip the epochs and batches to get the data loading at the same stage as in the previous training. If set to True, the training will begin faster (as that skipping step can take a long time) but will not yield the same results as the interrupted training would have.

- `output_dir` (str) 
    - 指定 model predictions and checkpoints 保存路径

- `overwrite_output_dir`  (bool, optional, defaults to False)
    - 目前没有直接被Trainer使用，是给用户做一些前置判断的。
    - If True, overwrite the content of the output directory. Use this to continue training if output_dir points to a checkpoint directory.

- `do_train` `do_eval` `do_predict` (bool, optional, defaults to False)
    - **不是直接被Trainer使用的，是给用户脚本中预留的**

- `prediction_loss_only`  (bool, optional, defaults to False) 
    - When performing evaluation and generating predictions, only returns the loss.
    - if prediction_loss_only: return (loss, None, None), else: return (loss, logits, labels)

- `per_device_train_batch_size` (int, optional, defaults to 8) 
    - The batch size per GPU/XPU/TPU/MPS/NPU core/CPU for training.
    - **Trainer中不是直接读取这个参数的，是根据args.train_batch_size 这个属性来间接获取的。**

- `per_device_eval_batch_size` (int, optional, defaults to 8) 
    - The batch size per GPU/XPU/TPU/MPS/NPU core/CPU for evaluation.

- `gradient_accumulation_steps` (int, optional, defaults to 1) 
    - Number of updates steps to accumulate the gradients for, before performing a backward/update pass.

- `eval_accumulation_steps` (int, optional)
    - Number of predictions steps to accumulate the output tensors for, before moving the results to the CPU. If left unset, the whole predictions are accumulated on GPU/NPU/TPU before being moved to the CPU (faster but requires more memory).

- `eval_delay` (float, optional) 
    - **控制第一次eval至少要 eval_delay个step或epoch后进行。**
    - Number of epochs or steps to wait for before the first evaluation can be performed, depending on the eval_strategy.

- `learning_rate` (float, optional, defaults to 5e-5) 
    - The initial learning rate for AdamW optimizer.

- `weight_decay` (float, optional, defaults to 0) 
    - The weight decay to apply (if not zero) to all layers except all bias and LayerNorm weights in AdamW optimizer.

- `max_grad_norm` (float, optional, defaults to 1.0) 
    - Maximum gradient norm (for gradient clipping).

- `num_train_epochs` (float, optional, defaults to 3.0) 
    - Total number of training epochs to perform (if not an integer, **will perform the decimal part percents of the last epoch before stopping training**).

- `max_steps` (int, optional, defaults to -1)
    - If set to a positive number, the total number of training steps to perform. Overrides num_train_epochs. For a finite dataset, training is reiterated through the dataset (if all data is exhausted) until max_steps is reached.

- `lr_scheduler_type` (str or SchedulerType, optional, defaults to "linear") 
    - The scheduler type to use. See the documentation of [SchedulerType](https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/optimizer_schedules#transformers.SchedulerType) for all possible values.

- `warmup_ratio` (float, optional, defaults to 0.0) 
    - Ratio of total training steps used for a linear warmup from 0 to learning_rate.

- `warmup_steps` (int, optional, defaults to 0) 
    - 在每个update_step进行操作
    - Number of steps used for a linear warmup from 0 to learning_rate. Overrides any effect of warmup_ratio.

- `log_level` (str, optional, defaults to passive) 
    - Logger log level to use on the main process. Possible choices are the log levels as strings: **‘debug’, ‘info’, ‘warning’, ‘error’ and ‘critical’**, plus a ‘passive’ level which doesn’t set anything and keeps the current log level for the Transformers library (which will be "warning" by default).

- `log_level_replica` (str, optional, defaults to "warning") 
    - Logger log level to use on replicas. Same choices as log_level”

- `log_on_each_node` (bool, optional, defaults to True)
    - In multinode distributed training, whether to log using log_level once per node, or only on the main node.

- `logging_dir` (str, optional)
    - TensorBoard log directory. Will default to *output_dir/runs/CURRENT_DATETIME_HOSTNAME*.

- `logging_steps` (int or float, optional, defaults to 500) 
    - Number of update steps between two logs if `logging_strategy="steps"`. Should be an integer or a float in range [0,1). If smaller than 1, will be interpreted as ratio of total training steps.

- `logging_strategy` (str or IntervalStrategy, optional, **defaults to "steps"**)
    - The logging strategy to adopt during training. Possible values are:  
        - "no": No logging is done during training.
        - "epoch": Logging is done at the end of each epoch.
        - "steps": Logging is done every logging_steps.

- `logging_first_step` (bool, optional, defaults to False)
    - **Whether to log the first global_step or not.**

- `logging_nan_inf_filter` (bool, optional, defaults to True) 
    - Whether to filter nan and inf losses for logging. **If set to True the loss of every step that is nan or inf is filtered and the average loss of the current logging window is taken instead.**


- `local_rank`(int, optional, defaults to -1)
    - Rank of the process during distributed training.

- `ddp_backend` (str, optional)
    - The backend to use for distributed training. Must be one of "nccl", "mpi", "ccl", "gloo", "hccl".

- `dataloader_drop_last` (bool, optional, defaults to False)
    - Whether to drop the last incomplete batch (if the length of the dataset is not divisible by the batch size) or not.

- `dataloader_num_workers` (int, optional, defaults to 0)
    - Number of subprocesses to use for data loading (PyTorch only). 0 means that the data will be loaded in the main process.

- `load_best_model_at_end` (bool, optional, defaults to False) 
    - Whether or not to load the best model found during training at the end of training. When this option is enabled, the best checkpoint will always be saved. See save_total_limit for more.

- `ignore_data_skip` (bool, optional, defaults to False) 
    - When resuming training, whether or not to skip the epochs and batches to get the data loading at the same stage as in the previous training. If set to True, the training will begin faster (as that skipping step can take a long time) but will not yield the same results as the interrupted training would have.

- `optim` (str or training_args.OptimizerNames, optional, defaults to "adamw_torch") 
    - The optimizer to use: adamw_hf, adamw_torch, adamw_torch_fused, adamw_apex_fused, adamw_anyprecision or adafactor.

- `report_to`(str or List[str], optional, defaults to "all")
    - The list of integrations to report the results and logs to. Supported platforms are "azure_ml", "clearml", "codecarbon", "comet_ml", "dagshub", "dvclive", "flyte", "mlflow", "neptune", "tensorboard", and "wandb". Use "all" to report to all integrations installed, "none" for no integrations.

- `resume_from_checkpoint` (str, optional)
    - **The path to a folder with a valid checkpoint for your model. This argument is not directly used by Trainer**, it’s intended to be used by your training/evaluation scripts instead. See the example scripts for more details.

- `gradient_checkpointing` (bool, optional, defaults to False)
    - If True, use gradient checkpointing to save memory at the expense of slower backward pass.

- `full_determinism` (bool, optional, defaults to False)
    - If True, enable_full_determinism() is called instead of set_seed() to ensure reproducible results in distributed training. Important: this will negatively impact the performance, so only use it for debugging.