
## 步骤
- 文档Trainer过一遍
- trainer结构梳理
- 总结
- run_clm梳理
- 文档中 TrainingArguments每个参数的怎么发挥作用
- 



## Traier入参
- `data_collator` (DataCollator, optional) — The function to use to form a batch from a list of elements of train_dataset or eval_dataset. **Will default to default_data_collator(). if no tokenizer is provided, an instance of DataCollatorWithPadding otherwise.**
- `tokenizer` (PreTrainedTokenizerBase, optional) — The tokenizer used to preprocess the data. If provided, **will be used to automatically pad the inputs to the maximum length when batching inputs**, and it will be saved along the model to make it easier to rerun an interrupted training or reuse the fine-tuned model.
- `model_init`(Callable[[], PreTrainedModel], optional) — A function that instantiates the model to be used. If provided, **each call to train() will start from a new instance of the model as given by this function.**

- `compute_metrics` #TODO
- `optimizers` (Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR], optional, defaults to (None, None)) — A tuple containing the optimizer and the scheduler to use. Will default to an instance of AdamW on your model and a scheduler given by `get_linear_schedule_with_warmup()` controlled by args.
- preprocess_logits_for_metrics #TODO

### 重要属性
- `place_model_on_device` — Whether or not to automatically place the model on the device - **it will be set to False if model parallel or deepspeed is used**, or if the default TrainingArguments.place_model_on_device is overridden to return False .



DefaultFlowCallback 设定了一些每个step和每个epoch结束时的行为

## Trainer怎么控制log, save, evaluate的
- 在训练的特殊节点都会尝试进行log_save_evaluate，是根据control变量的相关属性来决定是否分别做log_save_evaluate
- 而control变量的相关属性在 DefaultFlowCallback 的相关callback_step被调用时会被修改，例如on_step_end中会设置control.should_log, control.should_evaluate,而这些值的设定也是根据TrainingArgumens中的可配置参数设定的。
- 思考为什么要这么实现：**首先我们会通过TrainingArgumens设定什么时候进行log, save, evaluate；那么我们在Trainer.train的每个特殊节点都需要判断是否要进行log_save_evaluate，既然很多地方要重复写，那这三个操作可以抽象为一个函数_maybe_log_save_evaluate，但是这个函数要包含一个入参——时机，因为不同的时机这个函数的具体操作是不同的，比如step_end要log，epoch_end不要log；所以定义一个control变量来区分不同时机，不同的时机下control变量是怎么根据情况变化的呢？是通过callback_step让control在不同的时机产生不同的行为；callback_step在每个时机是靠什么决定control变化的呢，当然是根据最初的TrainingArguments设定的参数啦！**



## TrainingArguments参数梳理
- `logging_steps` (int or float, optional, defaults to 500) 
    - Number of update steps between two logs if `logging_strategy="steps"`. Should be an integer or a float in range [0,1). If smaller than 1, will be interpreted as ratio of total training steps.

- `logging_strategy` (str or IntervalStrategy, optional, **defaults to "steps"**)
    - The logging strategy to adopt during training. Possible values are:  
        - "no": No logging is done during training.
        - "epoch": Logging is done at the end of each epoch.
        - "steps": Logging is done every logging_steps.


- `eval_steps` (int or float, optional) 
    - Number of update steps between two evaluations if eval_strategy="steps". **Will default to the same value as logging_steps if not set.** Should be an integer or a float in range [0,1). If smaller than 1, will be interpreted as ratio of total training steps.

- `eval_strategy` (str or IntervalStrategy, optional, **defaults to "no"**)
    - The evaluation strategy to adopt during training. Possible values are:
        - "no": No evaluation is done during training.
        - "steps": Evaluation is done (and logged) every eval_steps.
        - "epoch": Evaluation is done at the end of each epoch.

- `save_steps` (int or float, optional, **defaults to 500**)
    - Number of updates steps before two checkpoint saves if save_strategy="steps". Should be an integer or a float in range [0,1). If smaller than 1, will be interpreted as ratio of total training steps.

- `save_strategy` (str or IntervalStrategy, optional, **defaults to "steps"**) 
    - The checkpoint save strategy to adopt during training. Possible values are:
        - "no": No save is done during training.
        - "epoch": Save is done at the end of each epoch.
        - "steps": Save is done every save_steps.
    - If "epoch" or "steps" is chosen, saving will also be performed at the very end of training, always.


- `max_steps` (int, optional, defaults to -1)
    - If set to a positive number, the total number of training steps to perform. Overrides num_train_epochs. For a finite dataset, training is reiterated through the dataset (if all data is exhausted) until max_steps is reached.

- `num_train_epochs` (float, optional, defaults to 3.0) 
    - Total number of training epochs to perform (if not an integer, will perform the decimal part percents of the last epoch before stopping training).

- `train_batch_size` 
    - 分布式训练时，就等于 `per_device_train_batch_size`


- `n_gpu` 
    - 当前进程使用的GPU数量，**分布式训练时，该属性总是固定为1**，只有在未使用分布式才可能大于1。
