
## 步骤
- 文档Trainer过一遍
- trainer结构梳理
- 总结
- run_clm梳理
- 文档中 TrainingArguments每个参数的怎么发挥作用
- 



## Traier入参
- `data_collator` (DataCollator, optional) — The function to use to form a batch from a list of elements of train_dataset or eval_dataset. **Will default to default_data_collator(). if no tokenizer is provided, an instance of DataCollatorWithPadding otherwise.**
- `tokenizer` (PreTrainedTokenizerBase, optional) — The tokenizer used to preprocess the data. If provided, **will be used to automatically pad the inputs to the maximum length when batching inputs**, and it will be saved along the model to make it easier to rerun an interrupted training or reuse the fine-tuned model.
- `model_init`(Callable[[], PreTrainedModel], optional) — A function that instantiates the model to be used. If provided, **each call to train() will start from a new instance of the model as given by this function.**

- `compute_metrics` #TODO
- `optimizers` (Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR], optional, defaults to (None, None)) — A tuple containing the optimizer and the scheduler to use. Will default to an instance of AdamW on your model and a scheduler given by `get_linear_schedule_with_warmup()` controlled by args.
- preprocess_logits_for_metrics #TODO

### 重要属性
- `place_model_on_device` — Whether or not to automatically place the model on the device - **it will be set to False if model parallel or deepspeed is used**, or if the default TrainingArguments.place_model_on_device is overridden to return False .



DefaultFlowCallback 设定了一些每个step和每个epoch结束时的行为